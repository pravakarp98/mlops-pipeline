# MLOps-pipeline
An end-to-end MLOps pipeline for predicting student academic risk (Graduate, Dropout, Enrolled). Features data versioning, experiment tracking (MLflow), hyperparameter tuning, FastAPI deployment, Docker containerization, and CI/CD automation with GitHub Actions.

## Student Academic Risk Predictor: End-to-End MLOps Project

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-0.68%2B-green)
![MLflow](https://img.shields.io/badge/MLflow-Tracking-orange)
![Docker](https://img.shields.io/badge/Docker-Container-blue)
![License](https://img.shields.io/badge/License-MIT-lightgrey)

## ğŸ“Œ Project Overview

This project implements a complete, production-grade machine learning pipeline to predict student academic risk in higher education. The model classifies students into three categories: **Graduate**, **Dropout**, or **Enrolled**.

It is built with a focus on MLOps best practices, demonstrating how to move from a raw dataset to a deployable, scalable API. The system includes automated training, hyperparameter tuning, experiment tracking, and containerized deployment.

### Key Features
* **Modular Codebase:** Clean separation of concerns (data loading, preprocessing, training, tuning, deployment).
* **Robust Preprocessing:** Custom feature engineering and `scikit-learn` pipelines for data transformation.
* **Experiment Tracking:** Integration with **MLflow** to log parameters, metrics, and model artifacts.
* **Hyperparameter Tuning:** Automated optimization using `RandomizedSearchCV`.
* **REST API:** A high-performance **FastAPI** application for real-time predictions.
* **Containerization:** Fully Dockerized application for consistent deployment.
* **CI/CD:** Automated build and push workflows using **GitHub Actions**.

## ğŸ“‚ Project Structure
student_risk_predictor/ 
â”œâ”€â”€ .github/ 
â”‚ â””â”€â”€ workflows/ 
â”‚ â””â”€â”€ ci-cd.yml # GitHub Actions workflow for CI/CD 
â”œâ”€â”€ app/ # FastAPI Application 
â”‚ â”œâ”€â”€ init.py 
â”‚ â”œâ”€â”€ main.py # API server logic 
â”‚ â””â”€â”€ schemas.py # Pydantic models for data validation 
â”œâ”€â”€ artifacts/ # Generated files (models, encoders, metrics) 
â”‚ â””â”€â”€ (Populated automatically by scripts) 
â”œâ”€â”€ data/ # Raw Data 
â”‚ â”œâ”€â”€ train.csv # Training dataset 
â”‚ â””â”€â”€ test.csv # Test dataset (optional) 
â”œâ”€â”€ mlruns/ # MLflow tracking data (auto-generated) 
â”œâ”€â”€ notebooks/ # Jupyter Notebooks 
â”‚ â””â”€â”€ 1-Data-Exploration.ipynb 
â”œâ”€â”€ src/ # Core ML Source Code 
â”‚ â”œâ”€â”€ init.py 
â”‚ â”œâ”€â”€ data_loader.py # Data loading and splitting logic 
â”‚ â”œâ”€â”€ preprocessor.py # Preprocessing pipeline definition 
â”‚ â”œâ”€â”€ train.py # Model training and selection script 
â”‚ â”œâ”€â”€ tune.py # Hyperparameter tuning script 
â”‚ â””â”€â”€ utils.py # Helper functions 
â”œâ”€â”€ .gitignore 
â”œâ”€â”€ Dockerfile # Docker image configuration 
â”œâ”€â”€ params.yaml # Configuration file for parameters 
â”œâ”€â”€ requirements.txt # Python dependencies 
â””â”€â”€ README.md # Project documentation

## ğŸš€ Getting Started

### Prerequisites
* Python 3.8+
* Git
* Docker (optional for local dev, required for containerization)

### Installation

1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/yourusername/student-risk-predictor.git](https://github.com/yourusername/student-risk-predictor.git)
    cd student-risk-predictor
    ```

2.  **Create and activate a virtual environment:**
    ```bash
    python -m venv venv
    # Windows
    venv\Scripts\activate
    # Mac/Linux
    source venv/bin/activate
    ```

3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Data Setup:**
    Ensure you have the `train.csv` file placed inside the `data/` directory.


## ğŸ› ï¸ Usage Pipeline

Follow these steps to reproduce the entire training and deployment process.

### 1. Data Exploration (Optional)
Run the Jupyter notebook to understand the dataset distribution and correlations.
```bash
# Open the notebook in your editor or Jupyter Lab
notebooks/1-Data-Exploration.ipynb
```